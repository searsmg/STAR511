---
title: 'STAR511: HW 8'
author: "Megan Sears"
output:
  word_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
#Retain (and do not edit) this code chunk!!!
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(here)
library(ggplot2)

```

# Q1

```{r}
#Q1
prestige <- read.csv('C:/Users/sears/Documents/Repos/STAR511/HW8/Prestige.csv', row.names=1)

pairs(prestige)

```

# Q2

```{r}
#Q2
cor(prestige)

```

# Q3

```{r}
#Q3
cor.test(x=prestige$income, y=prestige$prestige)

```

# Q4

```{r}
#Q4
summary(lm(prestige$prestige ~ prestige$income))

```

# Q5 

Based on the p-value (less than alpha) from Pearson's correlation and the correlation coefficient (0.71), there is a linear association between the two variables. Further, since there is a positive correlation (0.71), as income rises so does prestige. The null hypothesis, that there is no linear association or that the slope is 0, should be rejected based on the p-value from the correlation test and linear model (slope p-value). 

# Q6

The slope from the linear model is 0.0029 with an associated p-value less than 0.05. Therefore, for one dollar increase in income, estimate average prestige changes by 0.0029. 

# Q7

```{r}
#Q7
model <- lm(prestige$prestige ~ prestige$income)

par(mfrow=c(1,2))

plot(model, which = c(1:2))

```

# Q8

```{r}
#Q8
model2 <- lm(prestige ~ income + education, data = prestige)
summary(model2)

```

# Q9

The slope from the linear model is 0.0014 with an associated p-value less than 0.05. Therefore, for every dollar increase in income, prestige changes by 0.0014 assuming the value of education is held constant. 

# Q10

```{r}
#Q10 
par(mfrow=c(1,2))

plot(model2, which = c(1:2))

```

# Q11 

The R-squared value is 0.798. Therefore, approximately 80% of the variability found in prestige can be explained by the linear regression on income and education. 

# Q12

```{r}
#Q12
steel <- read_csv('C:/Users/sears/Documents/Repos/STAR511/HW8/Steel.csv')

ggplot(steel, aes(x=Thick, y=Strength)) +
  geom_point() + 
  labs(x='Coating Thickness') #+
  #geom_smooth(method = 'lm')

```

# Q13

```{r}
#Q13
steelmod <- lm(steel$Strength ~ steel$Thick)

par(mfrow=c(1,2))

plot(steelmod, which = c(1:2))

```

# Q14

No, the regression assumptions have not been met for equal variance and  normality. The lower and higher fitted values on the residuals versus fitted plot have smaller residual values than the rest causing an almost upside u-shape in the residual data. Additionally, based on the Q-Q plot, the data are left skewed, or lower values are not near the line. 

# Q15

```{r}
#Q15
steelmod_quad <- lm(Strength ~ Thick + I(Thick^2), data = steel)

summary(steelmod_quad)

```

# Q16

```{r}
#Q16
par(mfrow=c(1,2))

plot(steelmod_quad, which = c(1:2))

```

Based on the Q-Q plot from Q15, the data appear to be normal. This is an improvement from the left skew seen in the Q-Q plot from Q13. The residuals plot has larger spread for tail ends of the data, but appears to be an improvement from the the residuals versus fitted plot in Q13.

# Q17

```{r}
qplot(x = Thick, y = Strength, data = steel) +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = FALSE)

```




# Appendix
```{r show-code, ref.label = all_labels(), echo = TRUE, eval = FALSE}
```